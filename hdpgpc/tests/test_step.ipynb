{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This a brief tutorial to use HDP-GPC, a dynamic clustering library. \n",
    "As a summary of the objectives of this notebook:\n",
    "1. Learn to load some public data and learn how compute the initial statistics.\n",
    "2. Learn to define the model and check each one of the hyperparameters' configuration.\n",
    "3. Learn to execute inference and how this can be tuned.\n",
    "4. Learn how to plot the results and study the dynamic parameters."
   ],
   "id": "7f4f2effa776d8d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We should first import the main packages to deal with the data:",
   "id": "991af60910ad7c92"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we can load the data, to illustrate this example we are going to reduce the data to examples from 1700 to 1950.",
   "id": "becc84bbb5ab67ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cwd = os.path.dirname(os.getcwd())\n",
    "data_path = os.path.join(cwd, 'data', 'mitbih')\n",
    "data = np.load(os.path.join(data_path, '100.npy'))\n",
    "labels = np.load(os.path.join(data_path, '100_labels.npy'))\n",
    "\n",
    "print(\"Shape of the data: \")\n",
    "print(data.shape)\n",
    "data = data[1700:1950]\n",
    "labels = labels[1700:1950]\n",
    "num_samples, num_obs_per_sample, num_outputs = data.shape"
   ],
   "id": "393ef2e6a6649956",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The data dimensions correspond with [number of examples, number of observations, number of outputs]. Let's check how looks this data:",
   "id": "1b05046f092488f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(data[0,:,0])\n",
    "plt.show()"
   ],
   "id": "589279ea5959a7a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we can see, it corresponds to a heartbeat. This is the kind of patterns we want to cluster over the record.\n",
    "To compute the initial statistics we are going to use the observation variance and the 1-step rolled variance.\n",
    "For that, we have a method in the get_data class, where we can choose the batch to compute this statistics."
   ],
   "id": "dfc5ed994e11aad6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from hdpgpc.get_data import compute_estimators_LDS\n",
    "\n",
    "std, std_dif = compute_estimators_LDS(data, n_f=50)\n",
    "print(\"Variance: \", str(std))\n",
    "print(\"1-step variance: \", str(std_dif))"
   ],
   "id": "38eb197e100c3661",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we can see, the 1-step variance is higher than the variance, which indicates a slight dynamic behaviour.",
   "id": "5c8c666aa1a901a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we can define the HDP-GPC model. \n",
    "\n",
    "The hyperparameters can be directly estimated from the previously computed estimators. As a standard an initial M has to be chosen, as a default, it is chosen as M=2 as the model computes the needed number of clusters as it progresses in the inference.\n"
   ],
   "id": "dc3f7a3f5448fa3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "M = 2\n",
    "sigma = [std * 1.0] * M\n",
    "bound_sigma_ = (std * 0.1, std * 0.2)\n",
    "gamma = [std_dif * 1.0] * M\n",
    "bound_gamma = (std_dif * 0.1, std_dif * 1.0)"
   ],
   "id": "6cd659bd693a23bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can also define the kernel parameters, in this case we are taking outputscale = 300.0 as is the mean amplitude found on a standard record of ECG. Also, we are choosing ini_legthscale = 1.0 as a standard value. Both can be tuned, but they do not have a critical impact on the inference because they are optimized in terms of Maximum-Likelihood.",
   "id": "ca9395fa405e1ce0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "outputscale_ = 300.0\n",
    "ini_lengthscale = 3.0\n",
    "bound_lengthscale = (1.0, 20.0)"
   ],
   "id": "9e9a66ec5b593f61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Also, we have to define the time indexes where the observations had been taken, which is x_train. In this case it can be as simple as a range of the same length of the observations, but it can differ along the samples. On the other hand, we have to define x_basis, which is the time index basis where the LDS is going to be computed (it fix the dimension of the LDS) If desired, the x_basis can be reduced to be a set of inducing points. If this dimensional reduction is applied it should be reflected in the option parameter inducing_points=True. ",
   "id": "6880d8d87dc79619"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "samples = [0, num_obs_per_sample]\n",
    "l, L = samples[0], samples[1]\n",
    "x_basis = np.atleast_2d(np.arange(l, L, 1, dtype=np.float64)).T\n",
    "x_train = np.atleast_2d(np.arange(l, L, dtype=np.float64)).T\n",
    "#If x_basis is wanted to be smaller than the observations length, then the inducing points approach can be applied setting this parameter to True.\n",
    "inducing_points = False"
   ],
   "id": "580e60e46c4abc68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Additionally, we can also define the warp GP hyperparameters and time indexes. ",
   "id": "17d5ca3efdda0abb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Choose if warp is going to be applied. (In the most recent version is optimized to work with online inference, but it can be used in offline as an additional step at the end of the clustering).\n",
    "warp = False\n",
    "#Warp priors\n",
    "noise_warp = std * 0.1\n",
    "bound_noise_warp = (noise_warp * 0.1, noise_warp * 0.2)\n",
    "#Warp time indexes\n",
    "x_basis_warp = np.atleast_2d(np.arange(l, L, 2, dtype=np.float64)).T"
   ],
   "id": "fdbf8d5d3e4a8956",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Finally, with these parameters we can define the HDP-GPC. \n",
    "There exist a lot of configuration parameters that can be combined in this model, to check the options and the relevance of these configuration options go to the Documentation (in progress).\n",
    "\n",
    "In terms of define the model, we should consider the inference method we want to apply because each one have their restrictions (by now).\n",
    "\n",
    "1. In first place we have the online inference, where we can include one sample at a time and let the model cluster it. By now, the online inference only works with one output signal, but it is going to be extended soon. That's why we choose n_outputs=1."
   ],
   "id": "1eb063a6ee84c27d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import hdpgpc.GPI_HDP as hdpgp\n",
    "\n",
    "sw_gp = hdpgp.GPI_HDP(x_basis=x_basis, x_basis_warp=x_basis_warp, n_outputs=1,\n",
    "                          ini_lengthscale=ini_lengthscale, bound_lengthscale=bound_lengthscale,\n",
    "                          ini_gamma=gamma, ini_sigma=sigma, ini_outputscale=outputscale_, noise_warp=noise_warp,\n",
    "                          bound_sigma=bound_sigma_, bound_gamma=bound_gamma, bound_noise_warp=bound_noise_warp,\n",
    "                          verbose=True, max_models=100, inducing_points=inducing_points, estimation_limit=30)"
   ],
   "id": "6ece637dee735dfa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can revise the priors that had been defined for our model.  The cluster list of GP-LDS is generated inside the model, indexes as gpmodels[n_output][n_cluster]. We can plot the zero-knowledge cluster.",
   "id": "3c4329747de81f28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sw_gp.gpmodels[0][0].plot_last(0)",
   "id": "7be78c33272f3a55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "data_ = data[:,:,[0]]\n",
    "sw_gp.include_sample(x_train, data_[0], with_warp=warp)"
   ],
   "id": "3f7199f103dbee7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now the first example has been included in the model. \n",
    "We can check some parameters to be sure the sample has been added:"
   ],
   "id": "df1a9dafe8c92b82"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sw_gp.gpmodels[0][0].plot_last(0)",
   "id": "edc9368a9208d0a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we can see, the sample had been added successfully and the GP performed regression over it. Now we can include more samples, in this case, we are adding 100 to check how fast is the computation without warp component.",
   "id": "47f5ce640b8df331"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "for j, d in enumerate(data_[1:200]):\n",
    "    start_time = time.time()\n",
    "    print(\"Sample:\", j+1, \"/\", str(data_.shape[0]-1), \"label:\", labels[j+1])\n",
    "    sw_gp.include_sample(x_train, d, with_warp=warp)\n",
    "    print(\"Time --- %s seconds ---\" % (time.time() - start_time))"
   ],
   "id": "c9349384efdb034",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The clustering has generated two clusters. Let's revise them:",
   "id": "1f8ac3bd172748f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sw_gp.gpmodels[0][0].plot_last(0)\n",
    "sw_gp.gpmodels[0][1].plot_last(0)"
   ],
   "id": "374fc6efb2fdfceb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Even if we want to revise the beats included we can use the util_plot methods. Here the difference between groups is clearer:",
   "id": "b51db9b0921a20bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from hdpgpc.util_plots import print_results, plot_models_plotly\n",
    "\n",
    "main_model = print_results(sw_gp, labels, 0, error=False)\n",
    "selected_gpmodels = sw_gp.selected_gpmodels()\n",
    "plot_models_plotly(sw_gp, selected_gpmodels, main_model, labels, N_0=0, lead=0, step=0.5, plot_latent=True)"
   ],
   "id": "3d9cc429566bd9e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If we try to include the examples from 1900 to 1950, knowing that a ventricular beat exist among them, we are going to check if the model generates a new group.",
   "id": "853e66380d3ed510"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.plot(data_[200:,:,0].T)\n",
    "plt.show()\n"
   ],
   "id": "2795f470869e5690",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "for j, d in enumerate(data_[200:]):\n",
    "    start_time = time.time()\n",
    "    print(\"Sample:\", j + 200, \"/\", str(data_.shape[0]-1), \"label:\", labels[j+200])\n",
    "    sw_gp.include_sample(x_train, d, with_warp=warp)\n",
    "    print(\"Time --- %s seconds ---\" % (time.time() - start_time))"
   ],
   "id": "7ca09c74af08ac51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "main_model = print_results(sw_gp, labels, 0, error=False)\n",
    "selected_gpmodels = sw_gp.selected_gpmodels()\n",
    "plot_models_plotly(sw_gp, selected_gpmodels, main_model, labels, N_0=0, lead=0, step=0.5, plot_latent=True)"
   ],
   "id": "855ace340266977d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As expected, the ventricular beat generated a cluster itself.\n",
    "\n",
    "To perform the same clustering but in an offline scheme, we only have to choose the offline inference method, we are going to use it with full record. Here are some new option parameters: reestimate_initial_params indicates when a reestimation for the prior LDS is desired (it is recommended in the offline setting), n_explore_steps indicates the number of tryals to generate a new group on each iteration, recommended is above 10, but in this case to make the clustering faster, 3 will be enough. This full clustering takes a bit longer."
   ],
   "id": "f88483dc23b085b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sw_gp = hdpgp.GPI_HDP(x_basis=x_basis, x_basis_warp=x_basis_warp, n_outputs=1,\n",
    "                          ini_lengthscale=ini_lengthscale, bound_lengthscale=bound_lengthscale,\n",
    "                          ini_gamma=gamma, ini_sigma=sigma, ini_outputscale=outputscale_, noise_warp=noise_warp,\n",
    "                          bound_sigma=bound_sigma_, bound_gamma=bound_gamma, bound_noise_warp=bound_noise_warp,\n",
    "                          verbose=True, max_models=100, inducing_points=inducing_points, reestimate_initial_params=True,\n",
    "                          n_explore_steps=3)\n",
    "\n",
    "data = np.load(os.path.join(data_path, '100.npy'))[:,:,[0]]\n",
    "labels = np.load(os.path.join(data_path, '100_labels.npy'))\n",
    "num_samples = data.shape[0]\n",
    "x_trains = np.array([x_train] * num_samples)\n",
    "sw_gp.include_batch(x_trains, data)\n"
   ],
   "id": "79ac3e29126167d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we can plot the results same as before:",
   "id": "8874da874391f3b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "main_model = print_results(sw_gp, labels, 0, error=False)\n",
    "selected_gpmodels = sw_gp.selected_gpmodels()\n",
    "plot_models_plotly(sw_gp, selected_gpmodels, main_model, labels, N_0=0, lead=0, step=0.5, plot_latent=True)"
   ],
   "id": "964ede16ceb6485b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Of course, depending on the nature of the record to cluster, the hyperparameters can be tuned. First consideration should be focused on determining if the record is benefited of a dynamic clustering or not. In the case the patterns exhibit a dynamic change while remaining in the same category it will be interesting to priorize a dynamic behaviour, which is determined making $S_{\\omega} > S_{\\epsilon}$. If there exist different morphologies with patterns really close one to another, then it will be better to prioritize a static behaviour, with the opposite condition $S_{\\omega} < S_{\\epsilon}$. ",
   "id": "c88a34a358cb40ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Once we have the clustering performed, there are parameters of the model which can lead us to interesting insights. For example, the transition matrix represent the probability of jumping from one cluster to another, which can be useful to detect some prevalent loops of behaviour.",
   "id": "4bec028f2c220952"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sw_gp.compute_Pi()[:-1,:-1]\n",
   "id": "3c5c285caf9c7dbc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Also, visualizing the dynamic parameters can be useful in some cases:",
   "id": "6fc608c9ecfc76f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.imshow(sw_gp.gpmodels[0][0].Gamma[-1])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(sw_gp.gpmodels[0][0].Sigma[-1])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ],
   "id": "ee7279c9758cd0fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Usually, the dynamic transformation matrices $A, C$ are really close to the identity due to the conditions they have to satisfy (they have to be stable). So, sometimes is better to represent the transformation they produce in a vector of ones.",
   "id": "ae25bf2f662cbbe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.plot(torch.matmul(sw_gp.gpmodels[0][0].A[-1], torch.ones(x_basis.shape[0])))\n",
    "plt.plot(torch.matmul(sw_gp.gpmodels[0][1].A[-1], torch.ones(x_basis.shape[0])))\n",
    "plt.plot()"
   ],
   "id": "88509296767536c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here we can see how even if the normality clusters seems really similar, the underlying dynamic transformation is really different. Also, we can use the Kullback-Leibler divergence to measure the similarity of the models during their evolution and then represent this distances using a MDS reduction.",
   "id": "ab406f2f04de80a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from hdpgpc.util_plots import plot_MDS\n",
    "plot_MDS(sw_gp, main_model, labels, N_0=0, save=None)"
   ],
   "id": "ffb3ba00908b52a1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
