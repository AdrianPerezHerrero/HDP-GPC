{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This a brief tutorial to use HDP-GPC, a dynamic clustering library. \n",
    "As a summary of the objectives of this notebook:\n",
    "1. Learn to load some public data and learn how compute the initial statistics.\n",
    "2. Learn to define the model and check each one of the hyperparameters' configuration.\n",
    "3. Learn to execute inference and how this can be tuned.\n",
    "4. Learn how to plot the results and study the dynamic parameters."
   ],
   "id": "7f4f2effa776d8d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We should first import the main packages to deal with the data:",
   "id": "991af60910ad7c92"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we can load the data, to illustrate this example we are going to reduce the data to examples from 1800 to 1950.",
   "id": "becc84bbb5ab67ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cwd = os.path.dirname(os.getcwd())\n",
    "data_path = os.path.join(cwd, 'data', 'mitbih')\n",
    "data = np.load(os.path.join(data_path, '100.npy'))\n",
    "labels = np.load(os.path.join(data_path, '100_labels.npy'))\n",
    "\n",
    "print(\"Shape of the data: \")\n",
    "print(data.shape)\n",
    "num_samples, num_obs_per_sample, num_outputs = data.shape"
   ],
   "id": "393ef2e6a6649956",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The data dimensions correspond with [number of examples, number of observations, number of outputs]. Let's check how looks this data:",
   "id": "1b05046f092488f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(data[0,:,0])\n",
    "plt.show()"
   ],
   "id": "589279ea5959a7a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we can see, it corresponds to a heartbeat. This is the kind of patterns we want to cluster over the record.\n",
    "To compute the initial statistics we are going to use the observation variance and the 1-step rolled variance.\n",
    "For that, we have a method in the get_data class, where we can choose the batch to compute this statistics."
   ],
   "id": "dfc5ed994e11aad6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from hdpgpc.get_data import compute_estimators_LDS\n",
    "\n",
    "std, std_dif = compute_estimators_LDS(data, n_f=20)\n",
    "print(\"Variance: \", str(std))\n",
    "print(\"1-step variance: \", str(std_dif))"
   ],
   "id": "38eb197e100c3661",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we can see, the 1-step variance is higher than the variance, which indicates a slight dynamic behaviour.",
   "id": "5c8c666aa1a901a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we can define the HDP-GPC model. \n",
    "\n",
    "The hyperparameters can be directly estimated from the previously computed estimators. As a standard an initial M has to be chosen, as a default, it is chosen as M=2 as the model computes the needed number of clusters as it progresses in the inference.\n"
   ],
   "id": "dc3f7a3f5448fa3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "M = 2\n",
    "sigma = [std * 1.0] * M\n",
    "bound_sigma_ = (std * 0.1, std * 0.2)\n",
    "gamma = [std_dif * 1.0] * M\n",
    "bound_gamma = (std_dif * 0.1, std_dif * 1.0)"
   ],
   "id": "6cd659bd693a23bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can also define the kernel parameters, in this case we are taking outputscale = 300.0 as is the mean amplitude found on a standard record of ECG. Also, we are choosing ini_legthscale = 1.0 as a standard value. Both can be tuned, but they do not have a critical impact on the inference because they are optimized in terms of Maximum-Likelihood.",
   "id": "ca9395fa405e1ce0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "outputscale_ = 300.0\n",
    "ini_lengthscale = 3.0\n",
    "bound_lengthscale = (1.0, 20.0)"
   ],
   "id": "9e9a66ec5b593f61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Also, we have to define the time indexes where the observations had been taken, which is x_train. In this case it can be as simple as a range of the same length of the observations, but it can differ along the samples. On the other hand, we have to define x_basis, which is the time index basis where the LDS is going to be computed (it fix the dimension of the LDS) If desired, the x_basis can be reduced to be a set of inducing points. If this dimensional reduction is applied it should be reflected in the option parameter inducing_points=True. ",
   "id": "6880d8d87dc79619"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "samples = [0, num_obs_per_sample]\n",
    "l, L = samples[0], samples[1]\n",
    "x_basis = np.atleast_2d(np.arange(l, L, 1, dtype=np.float64)).T\n",
    "x_train = np.atleast_2d(np.arange(l, L, dtype=np.float64)).T\n",
    "#If x_basis is wanted to be smaller than the observations length, then the inducing points approach can be applied setting this parameter to True.\n",
    "inducing_points = False"
   ],
   "id": "580e60e46c4abc68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Additionally, we can also define the warp GP hyperparameters and time indexes. ",
   "id": "17d5ca3efdda0abb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Choose if warp is going to be applied. (In the most recent version is optimized to work with online inference, but it can be used in offline as an additional step at the end of the clustering).\n",
    "warp = False\n",
    "#Warp priors\n",
    "noise_warp = std * 0.1\n",
    "bound_noise_warp = (noise_warp * 0.1, noise_warp * 0.2)\n",
    "#Warp time indexes\n",
    "x_basis_warp = np.atleast_2d(np.arange(l, L, 2, dtype=np.float64)).T"
   ],
   "id": "fdbf8d5d3e4a8956",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Finally, with these parameters we can define the HDP-GPC. \n",
    "There exist a lot of configuration parameters that can be combined in this model, to check the options and the relevance of these configuration options go to the Documentation (in progress).\n",
    "\n",
    "In terms of define the model, we should consider the inference method we want to apply because each one have their restrictions (by now).\n",
    "\n",
    "1. In first place we have the online inference, where we can include one sample at a time and let the model cluster it. By now, the online inference only works with one output signal, but it is going to be extended soon. That's why we choose n_outputs=1."
   ],
   "id": "1eb063a6ee84c27d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import hdpgpc.GPI_HDP as hdpgp\n",
    "\n",
    "sw_gp = hdpgp.GPI_HDP(x_basis=x_basis, x_basis_warp=x_basis_warp, n_outputs=1,\n",
    "                          ini_lengthscale=ini_lengthscale, bound_lengthscale=bound_lengthscale,\n",
    "                          ini_gamma=gamma, ini_sigma=sigma, ini_outputscale=outputscale_, noise_warp=noise_warp,\n",
    "                          bound_sigma=bound_sigma_, bound_gamma=bound_gamma, bound_noise_warp=bound_noise_warp,\n",
    "                          verbose=True, max_models=100, inducing_points=inducing_points, estimation_limit=50)"
   ],
   "id": "6ece637dee735dfa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can revise the priors that had been defined for our model.  The cluster list of GP-LDS is generated inside the model, indexes as gpmodels[n_output][n_cluster]. We can plot the zero-knowledge cluster.",
   "id": "3c4329747de81f28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sw_gp.gpmodels[0][0].plot_last(0)",
   "id": "7be78c33272f3a55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "\n",
   "id": "802fe77dc9a4059b"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "data_ = data[:,:,[0]]\n",
    "sw_gp.include_sample(x_train, data_[0], with_warp=warp)"
   ],
   "id": "3f7199f103dbee7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now the first example has been included in the model. \n",
    "We can check some parameters to be sure the sample has been added:"
   ],
   "id": "df1a9dafe8c92b82"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sw_gp.gpmodels[0][0].plot_last(0)",
   "id": "edc9368a9208d0a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we can see, the sample had been added successfully and the GP performed regression over it. Now we can include more samples, in this case, we are adding 100 to check how fast is the computation without warp component.",
   "id": "47f5ce640b8df331"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "for j, d in enumerate(data_[1:100]):\n",
    "    start_time = time.time()\n",
    "    print(\"Sample:\", j, \"/\", str(data_.shape[0]-1), \"label:\", labels[j])\n",
    "    sw_gp.include_sample(x_train, d, with_warp=warp)\n",
    "    print(\"Time --- %s seconds ---\" % (time.time() - start_time))"
   ],
   "id": "c9349384efdb034",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As every sample has been included in the first cluster we can check the state of it.",
   "id": "1f8ac3bd172748f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sw_gp.gpmodels[0][0].plot_last(0)",
   "id": "374fc6efb2fdfceb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "79435ddfe4b657e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Even if we want to revise the beats included we can use the util_plot methods:",
   "id": "b51db9b0921a20bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from hdpgpc.util_plots import print_results, plot_models_plotly\n",
    "\n",
    "main_model = print_results(sw_gp, labels, 0, error=False)\n",
    "selected_gpmodels = sw_gp.selected_gpmodels()\n",
    "plot_models_plotly(sw_gp, selected_gpmodels, main_model, labels, N_0=0, lead=0, step=0.5, plot_latent=True)"
   ],
   "id": "3d9cc429566bd9e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If we try to include the examples from 1900 to 1950, knowing that a ventricular beat exist among them, we are going to check if the model generates a new group.",
   "id": "853e66380d3ed510"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "25909b90914aad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.plot(data_[1900:1910,:,0].T)\n",
    "plt.show()\n"
   ],
   "id": "2795f470869e5690",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "bc863dc60046c464"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "for j, d in enumerate(data_[1900:1910]):\n",
    "    start_time = time.time()\n",
    "    print(\"Sample:\", j + 1900, \"/\", str(data_.shape[0]-1), \"label:\", labels[j+1900])\n",
    "    sw_gp.include_sample(x_train, d, with_warp=warp)\n",
    "    print(\"Time --- %s seconds ---\" % (time.time() - start_time))"
   ],
   "id": "7ca09c74af08ac51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "main_model = print_results(sw_gp, labels, 0, error=False)\n",
    "selected_gpmodels = sw_gp.selected_gpmodels()\n",
    "plot_models_plotly(sw_gp, selected_gpmodels, main_model, labels, N_0=0, lead=0, step=0.5, plot_latent=True)"
   ],
   "id": "855ace340266977d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sw_gp.gpmodels[0][0].indexes\n",
   "id": "79ac3e29126167d6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
